---
title: "Data Quality Control: Validation"
output: html_document
date: '`r format(Sys.Date(), "%B %d, %Y")`'
---

```{r setup, include = FALSE}
library(dplyr)
library(ggplot2)
library(here)
library(testthat)
library(naniar)
library(tidyr)
library(tidytext)
library(readr)
library(pointblank)
library(janitor)
library(DT)

# note that echo has to be set TRUE to display pointblank output
knitr::opts_chunk$set(
  echo = FALSE, warning = FALSE, 
  fig.width = 10, fig.height = 14, dpi = 300
)

validate_rmd()
```

```{r funs, include = FALSE}
calculate_coverage <- function(indicator, id) {
  coverage_id <- n_distinct({{id}}[!is.na(indicator)])
  
  return(coverage_id)
}

calculate_time_range <- function(time_id){
  year_range <- paste0(min({{time_id}}, na.rm = TRUE), "-", max({{time_id}}, na.rm = TRUE))
  
  return(year_range)
}

compute_coverage <- function(data, country_id, year_id){
  data_coverage <- data |> 
    # compute (1) number of distinct country codes
    # (2) range of years covered
   summarise(
      across(
        c(everything()),
        list(
          country_coverage = ~ calculate_coverage(.x, {{country_id}}),
          year_coverage = ~ calculate_coverage(.x, {{year_id}}),
          year_range = ~ calculate_time_range({{year_id}}),
          prop_complete_records = ~ prop_complete(.x)
        ),
        .names = "{.col}__{.fn}"
      )
  ) |>
  pivot_longer(
      cols = c(everything()),
      cols_vary = "slowest",
      names_to = c("indicator", ".value"),
      names_pattern = "(\\w+)__(\\w+)"
  ) |> 
    arrange(
      indicator
    ) |> 
    select(
      Indicator = indicator,
      `Country Coverage` = country_coverage,
      `Year Coverage` = year_coverage,
      `Year Range` = year_range,
      `Proportion of Complete Records` = prop_complete_records
    )
  
  return(data_coverage)
}
```

# Introduction:

This document provides a protocol for data validation when manually importing data into CLIAR. The goal is to mitigate the following risks:

  1. Low data quality, measured in terms of country and year coverage.
  2. Change in data structure and indicator names over time.
  3. Inconsistency in country names and codes across data sources.

We provide an example of the data import workflow using the [ID4D database](https://id4d.worldbank.org/global-dataset).

# Steps for Validation of Manual Data Import:

  1. **Document data source:** For each data source, specify (a) the `url` to download the data, (b) where the data is stored and (c) when the data was downloaded.
  2. **Set expectations in a data import template:** To ensure consistency in data entry, there will be data import template for each data source. Each template specifies the indicators and data types.
  3. **Validate the data:** We use the package `pointblank` to validate the dataset. We specify rules to detect errors and inconsistencies in the data, such as missing or invalid values, incorrect formatting, or duplicates.
  4. **Review and approve the data import:** The diagnostic is reviewed and approved by a designated individual or team for each data import This ensures that the data meets the necessary review standards.
  5. **Store the data for each data source:** Imported data for each data source is stored in a designated location that is easily accessible to authorized individuals.

Data validation will be performed using the `R` packages: (1) [`pointblank`](https://rich-iannone.github.io/pointblank/) and (2) [`naniar`](https://naniar.njtierney.com/).

# Requirements

We specify a set of requirements that need to hold in order for the data to be imported.

  1. Data is [tidy](https://r4ds.had.co.nz/tidy-data.html). Namely: (1) indicators are in columns, (2) observations are in rows, and (3) values are in cells.
  2. Data has `country` and `year` identifiers.
  3. If any of the above does not hold, data has to be cleaned, and the data cleaning process will be documented in a separate script.

# How to Validate a Manual Import of Data:

Below, we provide a code example of how to validate a manual import of data, as applied to the ID4D data.

```{r, echo = FALSE}
id4d <- read_csv(
  here("..", "data", "raw", "data-quality-control", "id4d.csv")
)

id4d_clean <- id4d |> 
  clean_names()

# specify country and year id's with respect to the clean names
country_id <- "Country Code"
year_id <- "Time"
```


## 1. Document Data Source

This section specifies both the data source and location of the imported data file:

- Data Source: `https://databank.worldbank.org/source/identification-for-development-(id4d)-data`
- Data Location: `/path/to/data.csv`
- Data Import Date: `r format(Sys.Date(), "%B %d, %Y")`

Here we also specify what steps were taken during the import process, issues found and solutions implemented.

## 2. Set Expectations in a Data Import Template

This defines our expectations of the expected format of the data. This should specify both the type of indicator (e.g., `string`, `integer`),  as well as indicator names.

We verify that all relevant indicators are contained in the dataset, as specified by all variables contained in `vars_all`.

```{r validate = TRUE, echo = TRUE, eval = FALSE}
id4d_clean |>
  col_exists(
    columns = c(vars_all)
  )
```

## 3. Validate the Data

In this section, we define our expectations of the structure of the data. For example, whether it contains all the columns outlined in the `data_template` specified above. Additionally, whether it contains any missing values. 

These should be specified on a data source basis, but will be crucial for ensuring that we have validation procedures set in place.

In this example, we conduct validation tests on two sets of indicators: (1) Freedom House and (2) PEFA. The test is specified as: "No more than 10% of the values should be above a certain threshold". Note that because PEFA has so many missing values (96%), it fails its test.

```{r validation, validate = TRUE, echo = TRUE}
al <- action_levels(
  warn_at = 0.1
)

id4d_clean |> 
  create_agent() |>
  # verify that freedom-house columns are less than or equal to 7
  col_vals_lte(
    columns = contains("percent"),
    value = 100,
    label = "Verify Percentage Indicators are Less than 100",
    actions = al
  ) |>
  interrogate()
```

## 4. Quantitative Review of the Data

In this section we compute a set of diagnostics that assess the coverage of the dataset indicators. These diagnostics are:
  1. Country Coverage,
  2. Year Range,
  2. Year Coverage, and
  3. Proportion of Complete Records.
  
This section provides a set of diagnostic of all indicators.

```{r}
id4d_coverage <- id4d_clean |> 
  compute_coverage(
    country_id = country_code,
    year_id = time
  )

datatable(
  id4d_coverage,
  colnames = c("Indicator", "")
)
```


## 4. Review and approve the data import

In this review section, we document our interpretation of the diagnostics, as well as whether or not we have approved the data import. We should also specify who approved it.

## 5. Store the data

We should ideally store the approved data in a centralized data infrastructure, as discussed in our knowledge transfer meeting. We should avoid to the extent possible having local copies of the same data.

We also store the diagnostics, for quantitative review diagnostics, for future reference.

```{r, eval = FALSE}
# Store data
saveRDS(data_raw, "/path/to/data_raw.rds")
saveRDS(data_quality, "/path/to/data_quality.rds")
```


## Appendix 1: Full Summary of Indicators

**Note:** We should add information on the different sources for each one of the indicators. Additionally, there should be a common prefix

```{r echo = TRUE, eval = TRUE}
scan_data(
  id4d,
  sections = "OVS"
)
```
